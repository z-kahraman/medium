# ğŸš€ Real-Time Data Processing Pipeline

## ğŸ“– Introduction
This repository contains a **Real-Time Data Processing Pipeline** built using **Kafka, FastAPI, PostgreSQL, and Docker**. The project demonstrates how to:

âœ… **Produce real-time data streams using Kafka**  
âœ… **Consume and store processed data in PostgreSQL**  
âœ… **Implement a scalable microservices architecture with Docker**  
âœ… **Enable future real-time analytics with Spark Streaming**  

This project is ideal for **data engineers, backend developers, and real-time system enthusiasts**.

---

## ğŸ“‚ Project Structure
```
ğŸ“¦ real_time_pipeline
 â”£ ğŸ“‚ api
 â”ƒ â”£ ğŸ“œ producer.py      # Kafka Producer - Sends data to Kafka
 â”ƒ â”£ ğŸ“œ consumer.py      # Kafka Consumer - Reads data from Kafka and stores in PostgreSQL
 â”ƒ â”£ ğŸ“œ main.py          # FastAPI entry point
 â”£ ğŸ“‚ database
 â”ƒ â”£ ğŸ“œ db_connector.py  # PostgreSQL connection setup (Planned)
 â”ƒ â”£ ğŸ“œ models.py        # ORM models for database tables (Planned)
 â”£ ğŸ“‚ processing
 â”ƒ â”£ ğŸ“œ transform.py     # Data transformation functions (Planned)
 â”ƒ â”£ ğŸ“œ spark_stream.py  # Spark Streaming integration (Planned)
 â”£ ğŸ“œ docker-compose.yml # Manages Kafka, PostgreSQL services
 â”£ ğŸ“œ requirements.txt   # Python dependencies
 â”— ğŸ“œ README.md          # Documentation
```

---

## ğŸš€ Installation & Setup
### ğŸ”¹ 1. Clone the Repository
```bash
git clone https://github.com/z-kahraman/real_time_pipeline.git
cd real_time_pipeline
```

### ğŸ”¹ 2. Create a Virtual Environment & Install Dependencies
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### ğŸ”¹ 3. Run FastAPI Locally
```bash
uvicorn api.main:app --reload
```
FastAPI will be available at: **`http://127.0.0.1:8000/docs`**

---

## ğŸ³ Running with Docker
### ğŸ”¹ 1. Start Kafka & PostgreSQL in Docker
```bash
docker-compose up -d
```

### ğŸ”¹ 2. Verify Running Containers
```bash
docker ps
```

### ğŸ”¹ 3. Run FastAPI with Docker
```bash
docker build -t real-time-pipeline .
docker run -p 8000:8000 real-time-pipeline
```

---

## âš¡ Kafka Streaming Process
### ğŸ“Œ Produce Data
**Send data to Kafka**
```http
POST /produce/
```
Example Request:
```json
{
    "country": "Germany",
    "population": 83000000
}
```

### ğŸ“Œ Consume Data
**Read from Kafka and store in PostgreSQL**
```bash
python api/consumer.py
```

---

## ğŸ¯ Future Improvements
ğŸ”¹ Add **real-time analytics with Spark Streaming**  
ğŸ”¹ Implement **WebSocket for real-time dashboards**  
ğŸ”¹ Optimize **PostgreSQL with ORM & indexing**  

---

## ğŸ“œ License
This project is licensed under the MIT License.

---

## ğŸ“¢ Author
**Zafer Kahraman**  
ğŸ’¼ [LinkedIn](https://linkedin.com/in/zafer-kahraman)  
ğŸ“„ [Medium Blog](https://medium.com/@zafer_kahraman)  
ğŸ™ [GitHub](https://github.com/z-kahraman)  

ğŸš€ Follow for more **data engineering projects!**

